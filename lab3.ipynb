{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Logistic Regression </center>\n",
    "<center> Corinne Jones, TA </center>\n",
    "<center> DATA 558, Spring 2020 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we'll see how to use scikit-learn's logistic regression function. After this lab, you should know\n",
    "how to fit a logistic regression model with scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Logistic Regression \n",
    "Recall that in logistic regression we have a binary response variable $y$ and use the model\n",
    "$$ P(y=1|X; \\beta) = \\frac{\\exp{(\\beta_0+\\beta_1X_1 + \\cdots + \\beta_dX_d)}}{1+\\exp{(\\beta_0+\\beta_1X_1 + \\cdots + \\beta_dX_d)}}.$$\n",
    "By transforming the linear combination of predictors, $\\beta_0 + \\beta_1X_1 + \\cdots + \\beta_dX_d$, in the above equation we ensure that $P(y=1|X;\\beta)$ (the estimated probability that the response is equal to 1, conditional on the predictors $X=(X_1,\\dots,X_d)$) is always between 0 and 1. \n",
    "We can also rearrange the above equation to get\n",
    "$$ \\log\\left(\\frac{P(y=1|X; \\beta)}{1-P(y=1|X; \\beta)}\\right) = \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_dX_d.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the lecture you saw that when the labels are in the set $\\{-1, +1\\}$ (as opposed to, e.g.,  $\\{0, 1\\}$), maximizing the log-likelihood entails solving the problem \n",
    "$$\\min_{\\beta_0\\in\\mathbb{R}, \\beta \\in \\mathbb{R}^d} \\: \\frac{1}{n} \\sum_{i=1}^n \\log\\left(1+ \\exp(-y_i(\\beta_0+x_i^T \\beta))\\right).$$\n",
    "where $\\beta=[\\beta_1,\\dots, \\beta_d]^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the model we typically label a new input $x$ with 1 if our estimated probability of it being 1, $P(y=1|x; \\beta^\\star)$, is larger than the estimated probability of it being -1, $P(y=-1|x;\\beta^\\star)$ (or 0, depending on the convention). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** Denote by $\\beta_0^\\star$, $\\beta^\\star$ the $\\beta_0,\\beta$ that maximize the log-likelihood. Show that $P(y=1|x;\\beta^\\star)>P(y=-1|x;\\beta^\\star)$ if and only if $\\beta_0^\\star + (\\beta^\\star)^T x > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Log sum exp trick\n",
    "In practice, $\\exp(x_i^T\\beta)$ could be very large or very close to zero, resulting in overflow or underflow. Suppose we want to compute $\\log(1+\\exp(1000))$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.log(1+np.exp(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as though we get an overflow error! But it should be finite, since $1+\\exp(1000)\\approx \\exp(1000)$ and hence $\\log(1+\\exp(1000)) \\approx \\log(\\exp(1000)) = 1000$. To circumvent this problem, you can use a trick: the fact that\n",
    "$$ \\log(1+\\exp(x)) = a + \\log(\\exp(-a) + \\exp(x-a))$$\n",
    "for any $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** Show that the above identity is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.** Use the above identity to compute $\\log(1+\\exp(1000))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 800\n",
    "x = 1000\n",
    "a + np.log(np.exp(-a)+np.exp(x-a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get something close to 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Loading the data\n",
    "Next we'll use logistic regression to distinguish Hornbills and Toucans. The features that we'll use are derived from images. You can download the data from Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To do:** Change the directory below (if necessary) to load in the data for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_dir = 'lab3_data'\n",
    "\n",
    "x_train = np.load(os.path.join(data_dir, 'train_features.npy'))\n",
    "y_train = np.load(os.path.join(data_dir, 'train_labels.npy'))\n",
    "x_test = np.load(os.path.join(data_dir, 'test_features.npy'))\n",
    "y_test = np.load(os.path.join(data_dir, 'test_labels.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Hornbill\" has label 0 and \"Toucan\" has label 1. There are 500 images in the training set and 100 images in the test set. There is one *row* corresponding to each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 1000\n",
      "Dimension of features: 4096\n"
     ]
    }
   ],
   "source": [
    "print('Number of images:', x_train.shape[0])\n",
    "print('Dimension of features:', x_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Classification using Scikit-Learn\n",
    "Here we're going to use logistic regression from Scikit-learn to classify the images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.** Standardize the data. You may use Scikit-learn's `StandardScaler` for this. Use the same names for the normalized data (`x_train` and `x_test`).  \n",
    "Reference: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with linear regression, when using logistic regression it is beneficial to add a penalty term on the norm of the weights. With this penalty, we then optimize the expression\n",
    "$$\\min_{\\beta \\in \\mathbb{R}^d}\\; \\frac{1}{n} \\sum_{i=1}^n \\log\\left(1+ \\exp(-y_i(\\beta_0+x_i^T \\beta))\\right) + \\lambda \\|\\beta\\|^2_2.$$\n",
    "(where the labels are $\\pm 1$). We choose $\\lambda$ via cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.** Use `LogisticRegressionCV` to fit the model to the training data. For today use the default parameter values. Then compute the accuracy on the test set. Use `classifier` as the name of your instantiation of `LogisticRegressionCV`.\n",
    "Reference: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're able to get 88% accuracy! Let's get an idea of what images it classified correctly and incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "\n",
    "def display_ranked_image_list(names, image_dir, scores, num_images=10, cutoff=0.5, true_labels=None, display_mistakes=False):\n",
    "    \"\"\"\n",
    "    Display a (subset of a) ranked list of images. By default, this function displays\n",
    "    10 images from the list of image names (\"names\") sorted by decreasing scores (\"scores\").\n",
    "    :param names: List of image names\n",
    "    :param scores: Scores for each image (from some classifier)\n",
    "    :param num_images: Number of images to display\n",
    "    :param true_labels: The true labels of each image\n",
    "    :param display_mistakes: Whether to only display the top images on which the classifier made mistakes\n",
    "    \"\"\"\n",
    "    ncol = 6\n",
    "    \n",
    "    idxs = np.argsort(scores)\n",
    "    if not display_mistakes:\n",
    "        idxs = idxs[-num_images:]\n",
    "    else:\n",
    "        mistakes = np.where(true_labels != 1)[0]\n",
    "        idxs = [i for i in idxs if (i in mistakes and scores[i] > cutoff)][-num_images:]\n",
    "    \n",
    "    num_images = len(idxs)\n",
    "    nrow = int(num_images/6) + 1 if num_images % 6 != 0 else int(num_images/6)\n",
    "    fig = plt.figure()\n",
    "    fig.set_figwidth(15)\n",
    "    fig.set_figheight(5*nrow/2)\n",
    "    for i in range(1, num_images+1):\n",
    "        idx = idxs[i-1]\n",
    "        a = fig.add_subplot(nrow, 6, num_images-i+1)\n",
    "        img = mpimg.imread(os.path.join(image_dir, names[idx]))\n",
    "        imgplot = plt.imshow(img)\n",
    "        a.set_title(scores[idx])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of files in validation set\n",
    "image_names = sorted(os.listdir(os.path.join(data_dir, 'images', 'test')))\n",
    "image_dir = os.path.join(data_dir, 'images', 'test')\n",
    "\n",
    "# Generate estimated values for test observations using the logistic regression classifier\n",
    "test_probs = classifier.predict_proba(x_test)\n",
    "\n",
    "# See the images it was most confident were toucans that were in fact toucans\n",
    "print('Images it was most confident were toucans that were in fact toucans:')\n",
    "display_ranked_image_list(image_names, image_dir, test_probs[:, 1])\n",
    "\n",
    "#  See the images it was most confident about being toucans that were not toucans\n",
    "print('Images it was most confident about being toucans that were not toucans:')\n",
    "display_ranked_image_list(image_names, image_dir, test_probs[:, 1], true_labels=y_test, display_mistakes=True)\n",
    "\n",
    "# See the images it was most confident were hornbills\n",
    "print('Images it was most confident were hornbills that were in fact hornbills:')\n",
    "display_ranked_image_list(image_names, image_dir, 1-test_probs[:, 1])\n",
    "\n",
    "#  See the images it was most confident about being hornbills that were not hornbills\n",
    "print('Images it was most confident about being hornbills that were not hornbills:')\n",
    "display_ranked_image_list(image_names, image_dir, 1-test_probs[:, 1], true_labels=(y_test-1)*-1, display_mistakes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we've only discussed accuracy as a way of measuring the quality of a classifier. However, it turns out that there are also other ways to do so. We'll see these in a future lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
